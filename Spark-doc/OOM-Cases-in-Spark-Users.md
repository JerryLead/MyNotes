# OOM Cases in Spark
## [Apache Spark User List](http://apache-spark-user-list.1001560.n3.nabble.com/)

at [here](https://issues.apache.org/jira/browse/SPARK-1777?jql=project%20%3D%20SPARK%20AND%20text%20~%20%22oom%22)

at Spark-1777

Found 191 matching posts for `oom` in Apache Spark User [List](http://apache-spark-user-list.1001560.n3.nabble.com/template/NamlServlet.jtp?macro=search_page&node=1&query=oom).

Labels:

- Driver (Dr): error occurs in driver
- Executor (Ex): error occurs in executor
- Solved (S): this problem has been solved
- Unsolved (U):
	- R: we can reproduce the problem for further study  
	- D: just description which cannot be reproduced
- V: valuable 

## Very important
1. [Pass "cached" blocks directly to disk if memory is not large enough](https://issues.apache.org/jira/browse/SPARK-1777)

	Currently in Spark we entirely unroll a partition and then check whether it will cause us to exceed the storage limit. This has an obvious problem - if the partition itself is enough to push us over the storage limit (and eventually over the JVM heap), it will cause an OOM.
	
	New configurations.

	spark.storage.bufferFraction - the value of M as a fraction of the storage memory. (default: 0.2)
	
	spark.storage.safetyFraction - a margin of safety in case size estimation is slightly off. This is the equivalent of the existing spark.shuffle.safetyFraction. (default 0.9)
	
	For more detail, see the [design document](https://issues.apache.org/jira/secure/attachment/12651793/spark-1777-design-doc.pdf). Tests pending for performance and memory usage patterns.

2. [Changes to SizeEstimator more accurate](https://issues.apache.org/jira/browse/SPARK-385)

	This patch is motivated by an observation that the amount of heap space used by
the BoundedMemoryCache is often much larger than what we account for. 

	The object size and reference size are changed based on
the architecture in use and if or not CompressedOops are in use by the JVM. This
results in the object size changing from 8 to 12 or 16 and references being
either 4 or 8 bytes long. 

3. [Pluggable interface for shuffles](https://issues.apache.org/jira/browse/SPARK-2044)
	
	Discuss the sort-based shuffle

	


## Spark 
1. [trouble with "join" on large RDDs](http://apache-spark-user-list.1001560.n3.nabble.com/trouble-with-quot-join-quot-on-large-RDDs-td3864.html#a4039) (Ex, R)

	The join consistently crashes at the beginning of the reduce phase. 
	Note that when joining the 10G RDD to itself there is no problem. 
2. [Driver OOM while using reduceByKey](http://apache-spark-user-list.1001560.n3.nabble.com/Driver-OOM-while-using-reduceByKey-td6513.html) (Dr, S)

	too many tasks generated by reduceByKey() => control partitions => limit the number of reduce task
3. [OOM - Help Optimizing Local Job](http://apache-spark-user-list.1001560.n3.nabble.com/OOM-Help-Optimizing-Local-Job-td643.html) (Ex, R, V)

	HashMap used in user code.
4. [spark master OOME from maxMbInFlight buffers](http://apache-spark-user-list.1001560.n3.nabble.com/spark-master-OOME-from-maxMbInFlight-buffers-td1441.html) (Dr, D)

	The heap dump shows 70 byte[]s, owned by various Akka threads, all 48mb 
each (3.3gb total), which I assume is from the maxMbInFlight value. 
5. [GroupByKey results in OOM - Any other alternative](http://apache-spark-user-list.1001560.n3.nabble.com/GroupByKey-results-in-OOM-Any-other-alternative-td7625.html) (Ex, S, V)

	groupByKey().map( x => (x_1, x.\_2.distinct)).map(x => (x_1, x.\_2.distinct.count))
	
	Solution: You can do a little better than grouping *all* values and *then*  finding distinct values by using foldByKey(), putting values into a Set. Or countApproxDistinctByKey().
	
6. [Kyro serialization slow and runs OOM](http://apache-spark-user-list.1001560.n3.nabble.com/Kyro-serialization-slow-and-runs-OOM-td1073.html) (Ex, R)
	
	 I load my dataset, transform it with some one to one transformations, and try to cache the eventual RDD - it runs really slow and then runs out of memory. When I remove Kyro serializer and default back to java serialization it works just fine.
7. [OOM when calling cache on RDD with big data](http://apache-spark-user-list.1001560.n3.nabble.com/OOM-when-calling-cache-on-RDD-with-big-data-td1894.html) (Ex, R)

	I have a very simple job that simply caches the hadoopRDD by calling cache/persist on it. I tried MEMORY_ONLY, MEMORY_DISK and DISK_ONLY for caching strategy, I always get OOM on executors.
8. [how to set spark.executor.memory and heap size](http://apache-spark-user-list.1001560.n3.nabble.com/how-to-set-spark-executor-memory-and-heap-size-td4719.html#a7469)

	val logData = sc.parallelize(Array(1,2,3,4)).cache()
	
	Memory space is small.
9. [distinct on huge dataset](http://apache-spark-user-list.1001560.n3.nabble.com/distinct-on-huge-dataset-td3025.html#a3037) 
	
	I have a huge 2.5TB file. When i run: 
	
	val t = sc.textFile("/user/hdfs/dump.csv")   
	t.distinct.count 
10. [Does foreach operation increase rdd lineage?](http://apache-spark-user-list.1001560.n3.nabble.com/Does-foreach-operation-increase-rdd-lineage-td879.html#a881)

	Do you mean "Gibbs sampling" ? Actually, foreach is an action, it will collect all data from workers to driver. You will get OOM complained by JVM.
	
	Each update of the state of your markov chain should be a new RDD. I've found that I can do this for 100 or 200 iterations and then I'll get a stack overflow (presumably because the lineage is growing too large.) To get around this you either need to occasionally collect the RDD or write it to disk. Or just checkpoint() it.
	
11. [Fwd: Spark - ready for prime time?](http://apache-spark-user-list.1001560.n3.nabble.com/Fwd-Spark-ready-for-prime-time-td4064.html#a4183)

	The biggest issue I've come across is that the cluster is somewhat unstable when under memory pressure.  Meaning that if you attempt to persist an RDD that's too big for memory, even with MEMORY_AND_DISK, you'll often still get OOMs.  I had to carefully modify some of the space tuning parameters and GC settings to get some jobs to even finish.

	The other issue I've observed is if you group on a key that is `highly skewed`, with a few massively-common keys and a long tail of rare keys, the one massive key can be too big for a single machine and again cause OOMs.
	
	I agree with Andrew....Every time I `underestimate` the RAM requirement....my hand calculations are always ways less than what JVM actually allocates...
	
12. [Stream RDD to local disk](http://apache-spark-user-list.1001560.n3.nabble.com/Stream-RDD-to-local-disk-td1045.html)

	But if I .collect() on the driver and then save to disk using normal Scala disk IO utilities, I'll certainly OOM the driver.
13. [KafkaInputDStream mapping of partitions to tasks](http://apache-spark-user-list.1001560.n3.nabble.com/KafkaInputDStream-mapping-of-partitions-to-tasks-td3360.html#a3382)

	But with kafka, we can consume more rdds later, after we finish previous rdds. 
That way it would be much much simpler to not get OOM’ed when starting from beginning, 
because we can consume many data from kafka during batch duration and then get oom. 
14. [Questions about productionizing spark](http://apache-spark-user-list.1001560.n3.nabble.com/Questions-about-productionizing-spark-td4825.html)

	All went well except the last task of join. The job always stucked there, and lead eventually to OOM.  Even if we give sufficient memory and the job finally pass, the last task of join took significantly more time than other tasks (say several minutes vs 200ms). 
15. [Not getting it](http://apache-spark-user-list.1001560.n3.nabble.com/Not-getting-it-td3316.html#a3437)

	For the CSV file I used 1024 partitions [textFile(path, 1024)] which cut the partition size down to 8MB (based on standard HDFS 64MB splits).  For the key file I also adjusted partitions to use about 8MB.  This was still blowing up with GC Overlimit and Heap OOM with join.  I then set SPARK_MEM (which is hard to tease out of the documentation) to 4g and the join completed.
16. [newbie : java.lang.OutOfMemoryError: Java heap space](http://apache-spark-user-list.1001560.n3.nabble.com/newbie-java-lang-OutOfMemoryError-Java-heap-space-td365.html)

	mapper join with code.
	
	From the stack trace, it looks like the driver program is dying trying to serialize data out to the workers. 
	
17. [Using Spark on Data size larger than Memory size](http://apache-spark-user-list.1001560.n3.nabble.com/Using-Spark-on-Data-size-larger-than-Memory-size-td6589.html#a6642)

	In general, one problem with Spark today is that you can OOM under certain configurations, and it's possible you'll need to change from the default configuration if you're using doing very memory-intensive jobs. However, there are very few cases where Spark would simply fail as a matter of course -- for instance, you can always increase the number of partitions to decrease the size of any given one. or repartition data to eliminate skew.
	
18. [OutofMemory: Failed on spark/examples/bagel/WikipediaPageRank.scala](http://apache-spark-user-list.1001560.n3.nabble.com/OutofMemory-Failed-on-spark-examples-bagel-WikipediaPageRank-scala-td6040.html)

	I am running a 30GB Wikipedia dataset on a 7-server cluster. Using WikipediaPageRank underexample/Bagel.

	The problem is that the job will fail after several stages because of OutofMemory Error. The reason might be that the default executor's memory size is 512M.

19. [How to efficiently join this two complicated rdds](http://apache-spark-user-list.1001560.n3.nabble.com/How-to-efficiently-join-this-two-complicated-rdds-td1665.html#a1749)

	we have implement this way, we use pyspark, and standalone mode. We collect the new RDD2 in each iteration. The java heap memory costed by the driver program increases Gradually. And finally Collapse with OutOfMemory Error.
	
20. [OutOfMemoryError with basic kmeans](http://apache-spark-user-list.1001560.n3.nabble.com/OutOfMemoryError-with-basic-kmeans-td1651.html#a1653)

	I'm trying to run a basic version of kmeans (not the mllib version), on 250gb of data on 8 machines (each with 8 cores, and 60gb of ram).  I've tried many configurations, but keep getting an OutOfMemory error (at the bottom). 

21. [Serializer or Out-of-Memory issues?](http://apache-spark-user-list.1001560.n3.nabble.com/Serializer-or-Out-of-Memory-issues-td8533.html)

	ERROR TaskSchedulerImpl: Lost executor 0 on localhost: OutOfMemoryError
	
22. [Long running time for GraphX pagerank in dataset com-Friendster](http://apache-spark-user-list.1001560.n3.nabble.com/Long-running-time-for-GraphX-pagerank-in-dataset-com-Friendster-td4511.html#a4533)

	I was running some pagerank tests of GraphX in my 8 nodes cluster. I allocated each worker 32G memory and 8 CPU cores. The LiveJournal dataset used 370s, which in my mind is reasonable. But when I tried the com-Friendster data triggers out of memory.

23. [Partitioning - optimization or requirement?](http://apache-spark-user-list.1001560.n3.nabble.com/Partitioning-optimization-or-requirement-td3359.html)

	I'm often running out of memory when doing unbalanced joins (ie. cardinality of some joined elements much larger than others).  Raising the memory ceiling fixes the problem, but that's *a slippery slope*.
	
24. [trouble with broadcast variables on pyspark](http://apache-spark-user-list.1001560.n3.nabble.com/trouble-with-broadcast-variables-on-pyspark-td1301.html#a1308)

	The driver JVM is hitting OutOfMemoryErrors, but the python process is taking even more memory. 

25. [Exception in thread "DAGScheduler" java.lang.OutOfMemoryError: GC overhead limit exceeded](http://apache-spark-user-list.1001560.n3.nabble.com/Exception-in-thread-quot-DAGScheduler-quot-java-lang-OutOfMemoryError-GC-overhead-limit-exceeded-td833.html)
	
	I run a job that plans 3105 tasks. 3104 tasks out of 3105 tasks run OK, then the tasks start failing and after 36K failed tasks, I get following. Is the master running out of memory ?
26. [Spark streaming on load run - How to increase single node capacity?](http://apache-spark-user-list.1001560.n3.nabble.com/Spark-streaming-on-load-run-How-to-increase-single-node-capacity-td6953.html)
	Increased the number of working threads from local[2] to local[4] and local[8] and started getting: 
	 - outofmemoryerror: could not create native thread. 
	- RDD elements were being processed as empty values - as if the garbage collector or something inside Spark was cleaning them before time. 

27. [Memory allocation in the driver](http://apache-spark-user-list.1001560.n3.nabble.com/Memory-allocation-in-the-driver-td8406.html)

	If you read this and have heap allocation bothering you on calling "first". To get rid of the heap allocation error, increase the driver's memory or shrink size of first partition.
	
28. [Akka error with largish job (works fine for smaller versions)](http://apache-spark-user-list.1001560.n3.nabble.com/Akka-error-with-largish-job-works-fine-for-smaller-versions-td3097.html#a3209)
	
	all the workers ran out of memory.
	
	All I'm doing is data.map(r => (getKey(r),r)).sortByKey().map(_._2).coalesce(n).saveAsTextFile(), where n is the original number of files in the dataset.
	
	I took a look at a workers memory before it ran out using jmap and jhat; they indicated file handles as the biggest memory user (which I guess makes sense for a sort) - but the total was nowhere close to 200g
29. [Spark limitations question](http://apache-spark-user-list.1001560.n3.nabble.com/Spark-limitations-question-td3296.html)

	I have two tests: 
	1. join Base to itself, sum the "nums" and write out to HDFS 
	2. same as 1 except join Base to Skewed 
30. [Memory vs. disk - how to spill data into disk](http://apache-spark-user-list.1001560.n3.nabble.com/Memory-vs-disk-how-to-spill-data-into-disk-td1383.html)

	
31. [Spark stalling during shuffle (maybe a memory issue)](http://apache-spark-user-list.1001560.n3.nabble.com/Spark-stalling-during-shuffle-maybe-a-memory-issue-td6067.html)

	At this point Spark-related activity on the hadoop cluster completely halts .. there's no network activity, disk IO or CPU activity, and individual tasks are not completing and the job just sits in this state.  At this point we just kill the job & a re-start of the Spark server service is required. 
	
	So we upped the spark.akka.frameSize value to 128 MB and still observed the same behavior.  It's happening not necessarily when data is being sent back to the driver, but when there is an inter-cluster shuffle, for example during a groupByKey.
	
	If the distribution of the keys in your groupByKey is skewed (some keys appear way more often than others) you should consider modifying your job to use reduceByKey instead wherever possible.
32. [pyspark join crash](http://apache-spark-user-list.1001560.n3.nabble.com/pyspark-join-crash-td6938.html)

	In PySpark, the data processed by each reduce task needs to fit in memory within the Python process, so you should use more tasks to process this dataset. Data is spilled to disk across tasks. 
	
	I think the problem is that once unpacked in Python, the objects take considerably more space, as they are stored as Python objects in a Python dictionary. 
	
	I’m not sure whether there’s a great way to inspect a Python process’s memory, but looking at what consumes memory in a reducer process would be useful. 
	
33. [FileNotFoundException when using persist(DISK_ONLY)](http://apache-spark-user-list.1001560.n3.nabble.com/FileNotFoundException-when-using-persist-DISK-ONLY-td7291.html)

	I'm thinking that the FileNotFoundExceptions are due to tasks being cancelled/restarted and the root cause is the OutOfMemoryError.
	
	Otherwise, I figure next steps would be to enable more debugging levels in the spark code to see what much memory the code is trying to allocate. At this point, I'm wondering if the block could be in the GB range.

34. [spark streaming rate limiting from kafka](http://apache-spark-user-list.1001560.n3.nabble.com/spark-streaming-rate-limiting-from-kafka-td8590.html#a8592)

	In my use case, if I need to stop spark streaming for a while, data would accumulate a lot on kafka topic-partitions. After I restart spark streaming job, the worker's heap will go out of memory on the fetch of the 1st batch.

35. [advice on maintaining a production spark cluster?](http://apache-spark-user-list.1001560.n3.nabble.com/advice-on-maintaining-a-production-spark-cluster-td5848.html#a6124)

	We've had occasional problems with running out of memory on the driver side (esp. with large broadcast variables) so that may be related.  

36. [Spark Processing Large Data Stuck](http://apache-spark-user-list.1001560.n3.nabble.com/Spark-Processing-Large-Data-Stuck-td8075.html#a8077)
	
	I run the pagerank example processing a large data set, 5GB in size, using 48 machines. The root cause is the out of memory error - verified this by monitoring the memory. 
	
37. [Problems with broadcast large datastructure](http://apache-spark-user-list.1001560.n3.nabble.com/Problems-with-broadcast-large-datastructure-td331.html#a491)
	
	If your object size > 10MB you may need to change spark.akka.frameSize.

	400MB isn't really that big. Broadcast is expected to work with several GB of data and in even larger clusters (100s of machines).
	
	if you are using the default HttpBroadcast, then akka isn't used to move the broadcasted data. But block manager can run out of memory if you repetitively broadcast large objects. Another scenario is that the master isn't receiving any heartbeats from the blockmanager because the control messages are getting dropped due to bulk data movement. Can you provide a bit more details on your network setup?
	
38. [shuffle memory requirements](http://apache-spark-user-list.1001560.n3.nabble.com/shuffle-memory-requirements-td4048.html#a4133)

	val hrdd = sc.hadoopRDD(..)
	
	val res = hrdd.partitionBy(myCustomPartitioner).reduceKey(..).mapPartitionsWithIndex( some code to save those partitions )

	 I'm getting OutOfMemoryErrors on the read side of partitionBy shuffle. My custom partitioner generates over 20,000 partitions, so there are 20,000 tasks reading the shuffle files. On problems with low partitions (~ 1000), the job completes successfully. 
	
39. [extremely slow k-means version](http://apache-spark-user-list.1001560.n3.nabble.com/extremely-slow-k-means-version-td4489.html#a4492)

	Now, why I'm here for, this version runs EXTREMELY slow and gets outOfHeapMemory exceptions for data input that the original algorithm easily solves in ~5seconds. I'm trying to pinpoint what exactly is causing this huge difference. 

	This means that a) all points will be sent across the network in a cluster, which is slow (and Spark goes through this sending code path even in local mode so it serializes the data), and b) you’ll get out of memory errors if that Seq is too big. 
	
40. [Help: WARN AbstractNioSelector: Unexpected exception in the selector loop. java.lang.OutOfMemoryError: Java heap space](http://apache-spark-user-list.1001560.n3.nabble.com/Help-WARN-AbstractNioSelector-Unexpected-exception-in-the-selector-loop-java-lang-OutOfMemoryError-Je-td8633.html#a8637)

	It seems that the driver program gets out of memory. 
In Windows Task Manager, the driver program's memory constantly grows until 
around 3,434,796, then java OutOfMemory exception occurs. 

41. [Do not materialize partitions when DISK_ONLY storage level is used](https://issues.apache.org/jira/browse/SPARK-942)
	
	If an operation returns a generating iterator (i.e. one that creates return values as the 'next' method is called), for example as the result of a 'flatMap' call on an RDD, the CacheManager first completely unrolls the iterator into an Array buffer before passing it to the blockManager (CacheManager.scala:74). Only after the entire iterator has been put into a buffer does it check if there is enough space in memory to store the data (BlockManager.scala:608). 
In the attached test, the code can complete the operation of 'saveAsTextFile' of text strings if it is called directly on the result RDD of a flatMap operation, this is because it is given an iterator result, and works on the map-then-save operation as the results are generated. In the other branch, a 'persist' is called, and the cacheManger first tries to un-roll the entire iterator before deciding to store it too disk, this will cause a Memory Error (on systems with -Xmx512m)

42. [ExternalAppendOnlyMap can still OOM if one key is very large](https://issues.apache.org/jira/browse/SPARK-1823)

	If the values for one key do not collectively fit into memory, then the map will still OOM when you merge the spilled contents back in.
	
43. [Support external sorting for RDD#sortByKey()](https://issues.apache.org/jira/browse/SPARK-983)

	Currently, RDD#sortByKey() is implemented by a mapPartitions which creates a buffer to hold the entire partition, then sorts it. This will cause an OOM if an entire partition cannot fit in memory, which is especially problematic for skewed data. Rather than OOMing, the behavior should be similar to the ExternalAppendOnlyMap, where we fallback to disk if we detect memory pressure.
	
44. [Reduce memory footprint of DiskBlockManager.blockToFileSegmentMap](https://issues.apache.org/jira/browse/SPARK-946)

	blockToFileSegmentMap right now is taking up ~400 bytes per FileSegment. In large shuffles (e.g., >1000 mappers/executor and >1000 reducers), this can lead to several GB used just for this map, which is leading to OOM
	
45. [GraphX should have messageRDD to enable OutOfCore messages](https://issues.apache.org/jira/browse/SPARK-1531)
	
	For some message intensively computation on some bigger graph, it will throw OOM exceptions.
	
46. [Driver program should not put a block in memory](https://issues.apache.org/jira/browse/SPARK-821)

	Often the driver/master node has ram allocated than the worker nodes.
	
	In the case the user runs take or first on a cached RDD, the task can get launched locally on the master, and then the master would attempt to put the first block (or first few blocks) in memory, leading to OOM on the master.
	
47. [DiskStore should use > 8kB buffer when doing writes](https://issues.apache.org/jira/browse/SPARK-741)

	If this means each split buffer is < 8kB, bump up to at least 8kB (we'd rather OOM then have terrible disk throughput, so at least people can figure out what's wrong)
	

	





	

##Spark (D)
1. [com.google.protobuf out of memory](http://apache-spark-user-list.1001560.n3.nabble.com/com-google-protobuf-out-of-memory-td6357.html#a6373) (D)
	
	I am getting a OutOfMemoryError in class ByteString.java from package com.google.protobuf when processing very large data using spark 0.9. 

2. [Kafka streaming out of memory](http://apache-spark-user-list.1001560.n3.nabble.com/Kafka-streaming-out-of-memory-td2639.html) (D)

	 When I use the kafka streaming with spark, the rdds store in memory and never be realesed, so OOM will happen after time. 	

3. [Maximum memory limits](http://apache-spark-user-list.1001560.n3.nabble.com/Maximum-memory-limits-td2717.html) (D)

	I was thinking that jblas is going to call native malloc right? 
4. [Pyspark Memory Woes](http://apache-spark-user-list.1001560.n3.nabble.com/Pyspark-Memory-Woes-td2538.html#a4093) (D)

	We're seeing various OOM problems: sometimes python takes all available mem, sometimes we OOM with no heap space left, and occasionally OOM with GC overhead limit exceeded.  I realise it's hard to troubleshoot in the absence of code but any test case we have would be contrived. It seems we should be setting it to (SPARK_WORKER_MEMORY + pyspark memory) / # of concurrent applications, but is there any advice on how to balance memory between executors and pyspark, or does it depend too much on the workload? How do we know if we're getting the most bang for our buck, so to speak?
5. [Spark 0.9.1 java.lang.outOfMemoryError: Java Heap Space](http://apache-spark-user-list.1001560.n3.nabble.com/Spark-0-9-1-java-lang-outOfMemoryError-Java-Heap-Space-td7861.html)

	I am trying to process a file that contains 4 log lines (not very long) and then write my parsed out case classes to a destination folder, and I get the following error:
	


## MLlib (E)
1.  [Running out of memory Naive Bayes](http://apache-spark-user-list.1001560.n3.nabble.com/Running-out-of-memory-Naive-Bayes-td4866.html#a4876) (E)
	
	Each example in the dataset is about 2 million features, only about 20-50 of which are non-zero, so the vectors are very sparse. I keep running out of memory though, even for about 1000 examples on 30gb RAM while the entire dataset is 4 million examples.
	
## Spark Streaming
1. [Out of memory when spark streaming](http://apache-spark-user-list.1001560.n3.nabble.com/help-me-Out-of-memory-when-spark-streaming-td5854.html)

	I send data to spark streaming through Zeromq at a speed of 600 records per second, but the spark streaming only handle 10 records per 5 seconds( set it in streaming program)
2. [Any advice for using big spark.cleaner.delay value in Spark Streaming?](http://apache-spark-user-list.1001560.n3.nabble.com/Any-advice-for-using-big-spark-cleaner-delay-value-in-Spark-Streaming-td4895.html#a4909)

	Spark Streaming is most useful when you want the processing results based on incoming data streams within seconds of receiving the data. In case, you want to do aggregations across a day's data and do it in real time and continuously (e.g. every 5 second, count records received in last 1 day), then you probably have to do something a little bit smarter - have per-10-minute / per-hour counts, which gets continuously together with the latest partial-hour counts.
	
## System-related issues
1. [PySpark runs out of memory with large broadcast variables](https://issues.apache.org/jira/browse/SPARK-1065)

	PySpark's driver components may run out of memory when broadcasting large variables (say 1 gigabyte).
2. [Spark runs out of memory on fork/exec (affects both pipes and python)](https://issues.apache.org/jira/browse/SPARK-671)

	Because the JVM uses fork/exec to launch child processes, any child process initially has the memory footprint of its parent. In the case of a large Spark JVM that spawns many child processes (for Pipe or Python support), this quickly leads to kernel memory exhaustion.
3. [Local spark-shell Runs Out of Memory With Default Settings](https://issues.apache.org/jira/browse/SPARK-1392)
	
	running the spark-shell locally in out of the box configuration, and attempting to cache all the attached data, spark OOMs with: java.lang.OutOfMemoryError: GC overhead limit exceeded
You can work around the issue by either decreasing spark.storage.memoryFraction or increasing SPARK_MEM.
4. [ExternalAppendOnlyMap can still OOM if one key is very large](https://issues.apache.org/jira/browse/SPARK-1823)

	If the values for one key do not collectively fit into memory, then the map will still OOM when you merge the spilled contents back in.

	This is a problem especially for PySpark, since we hash the keys (Python objects) before a shuffle, and there are only so many integers out there in the world, so there could potentially be many collisions.
	
5. [2GB limit in spark for blocks](https://issues.apache.org/jira/browse/SPARK-1476)

	The underlying abstraction for blocks in spark is a ByteBuffer : which limits the size of the block to 2GB with [proposal](https://issues.apache.org/jira/secure/attachment/12652276/2g_fix_proposal.pdf)

	



## Memory-related issues
1. [Spark Memory Bounds](http://apache-spark-user-list.1001560.n3.nabble.com/Spark-Memory-Bounds-td6456.html#a6500) (VD)

	Spark's memory usage is roughly the following:

	(A) In-Memory RDD use + (B) In memory Shuffle use + (C) Transient memory used by all currently running tasks.

2. [Pyspark Memory Woes](http://apache-spark-user-list.1001560.n3.nabble.com/Pyspark-Memory-Woes-td2538.html)

	We're seeing various OOM problems: sometimes python takes all available mem, sometimes we OOM with no heap space left, and occasionally OOM with GC overhead limit exceeded.
3. [ExternalAppendOnlyMap: Spilling in-memory map](http://apache-spark-user-list.1001560.n3.nabble.com/ExternalAppendOnlyMap-Spilling-in-memory-map-td6186.html#a6221)

	 The ExternalAppendOnlyMap is used when a shuffle is causing too much data to be held in memory.  Rather than OOM'ing, Spark writes the data out to disk in a sorted order and reads it back from disk later on when it's needed.  That's the job of the ExternalAppendOnlyMap.
4. [Local Standalone Application and shuffle spills](http://apache-spark-user-list.1001560.n3.nabble.com/Local-Standalone-Application-and-shuffle-spills-td2634.html#a2680)

	"spark.shuffle.spill" refers to a different behavior -- if the "reduce" phase of your shuffle would otherwise cause Spark to OOM, it will instead write data to temporary files on disk. You probably don't want to disable this unless you'd prefer to tune Spark to make sure the reduce can stay in memory.
5. [Performance and serialization: use case](http://apache-spark-user-list.1001560.n3.nabble.com/Performance-and-serialization-use-case-td1513.html#a1615)
	- SERIALIZATION: 
		- Is it OK to manipulate RDDs of Record objects or should we stick with simple RDDs of strings, and do all the splitting and computation in each transformation ? 
		- How to make that efficient in terms of memory and speed? 
		- I've read the docs about the Tuning (and Kryo serialization) but I'd like to have more info on that... 
	- PERFORMANCE: 
		- Is it a good idea to perform all the filters first, and then the groupBy customer, or should we do the reverse? 
		- In the second situation, how can we filter on the values? I didn't see a filterValues method in the PairRDD API ? 
6. [computation slows down 10x because of cached RDDs](http://apache-spark-user-list.1001560.n3.nabble.com/computation-slows-down-10x-because-of-cached-RDDs-td2480.html)

	I think what happened is the following: all the nodes generated some garbage that put them very close to the threshold for a full GC in the first few runs of the program (when you cached the RDDs), but on the subsequent queries, only a few nodes are hitting full GC per query, so every query sees a slowdown but the problem persists for a while.
7. [OutOfMemoryErrors can cause workers to hang indefinitely](https://issues.apache.org/jira/browse/SPARK-599)

	Handling and reporting failures due to OutOfMemoryError can be complicated because the OutOfMemoryError exception can be raised at many different locations, depending on which allocation caused the error.
	
	I think we might want to deal with this using a timeout instead of trying to catch the error. Do you know whether any threads continue running at all when there's an OOM? Somehow I doubt it.
8. [web ui stage page becomes unresponsive when the number of tasks is large](https://issues.apache.org/jira/browse/SPARK-2017)

	Currently, for our jobs, I run with spark.ui.retainedStages=3 (so that there is some visibility into past stages) : this is to prevent OOM's in the master when number of tasks per stage is not low (50k for example is not very high imo)
	
	
	

## Interesting questions

1. Is there a way to know which key,value pair is resulting in the OOM?
2. Is there a way to set parallelism in the map stage so that each worker will process one key at time?
3. I was wondering how Spark handles congestion when the upstream is generating dstreams faster than downstream workers can handle? It will eventually OOM.
4. Is there a reason that this value couldn't be dynamically adjusted in response to actual heap usage?
5. Is there a way to automatically re-spawn spark workers? We've situations where executor OOM causes worker process to be DEAD and it does not came back automatically. 
6. Why am I seeing OOMs to begin with? 
7.  How can I reduce the likelihood of seeing OOMs 
8.  Why does an OOM seem to break the executor so hopelessly?
9.  By default 66% of the executor memory is used for RDD caching, so if there's no explicit caching in the code (eg. rdd.cache(), rdd.persiste(StorageLevel.MEM_AND_DISK) etc), this ram is wasted?
10. Is it possible to get the memory usage of one single task in JVM with GC running in the background? 

	you could run 1-core slaves. That way they would only work on one task at a time.
	
	The way I understand it, Spark does not have a tight control on the memory. Your code running on the executor can easily use more than 40% of memory. Spark only limits the memory used for RDD caches and shuffles. If its RDD caches are full, taking up 60% of the heap, and your code takes up more than 40% (after GC), the executor will die with OOM.
	
	heap dump => thread analysis 
	
	Try [heapaudit](https://github.com/foursquare/heapaudit)

11. I want to know that in the case when the size of HBase Table grows larger than the size of RAM available in the cluster, will the application fail, or will there be an impact in performance?
12. What I can not understand is there any easy way to say when we have out of memory situation please spill data into disk.

## Solutions:
1. If the foldByKey solution doesn't work for you, my team uses RDD.persist(DISK_ONLY) to avoid OOM errors.
2. Is your RDD of Strings?  If so, you should make sure to use the Kryo serializer instead of the default Java one.  It stores strings as UTF8 rather than Java's default UTF16 representation, which can save you half the memory usage in the right situation.
3. If an individual partition becomes too large to fit in memory then the usual approach would be to repartition to more partitions, so each one is smaller. Hopefully then it would fit.
4. resource contention. -Xmx cannot be achieved.
5. If you are running a local standalone Spark cluster, you can set the amount of memory that the worker can use by setting `spark.executor.memory`.
6. You can increase the number of partitions created from the text file, and increase the number of reducers. This lowers the memory usage of each task. See Spark tuning guide.
7. If you are still running out of memory, you can try our latest Spark 0.9 (or the master branch). Shuffles have been modified to automatically spill to disk when it needs more than available memory. 0.9 is undergoing community voting and will be released very soon.


## Knowledge
1. When you launch each job, you set SPARK_MEM in its environment to control executors' memory usage. So for example SPARK_MEM=10g java -jar MyCode.jar. SPARK_MEM will automatically be propagated to the slaves. Although you can also set SPARK_MEM in conf/spark-env.sh, it's recommended not to do this, because that would set the same value for every job; the ability to set it there is just left-over from previous versions of Spark and we eventually want to remove it. (I realize it's confusing though.) 

2. SPARK_WORKER_MEMORY is only there to tell each worker daemon the *total* amount of memory on the machine, so that it can know how many concurrent jobs it can safely run (based on each job's SPARK_MEM). In 99% of cases, you will not have to set SPARK_WORKER_MEMORY; the Worker process automatically detects the amount of memory available. 

3. [KafkaInputDStream mapping of partitions to tasks](http://apache-spark-user-list.1001560.n3.nabble.com/KafkaInputDStream-mapping-of-partitions-to-tasks-td3360.html#a3378)
4.  In general, one problem with Spark today is that you can OOM under certain configurations, and it's possible you'll need to change from the default configuration if you're using doing very memory-intensive jobs. However, there are very few cases where Spark would simply fail as a matter of course -- for instance, you can always increase the number of partitions to decrease the size of any given one. or repartition data to eliminate skew.
5.  groupByKey is an expensive transformation, and collecting all the data to driver side may simply cause OOM if the data can’t fit in the driver node.
6.  If your job is using external sorting to avoid OOMing (which it will warn you about in the executor logs with messages like "Spilling in-memory map..."), then you may have arbitrarily many files open. This is very unlikely to happen if you've split your input into as many files as you said, though.
7.  The more I think about it the problem is not about /tmp, its more about the workers not having enough memory. Blocks of received data could be falling out of memory before it is getting processed. BTW, what is the storage level that you are using for your input stream? If you are using MEMORY_ONLY, then try MEMORY_AND_DISK. That is safer because it ensure that if received data falls out of memory it will be at least saved to disk.
8. "In all cases, we recommend allocating only at most 75% of the memory for Spark; leave the rest for the operating system and buffer cache."
9. log of JVM

		# Native memory allocation (malloc) failed to allocate 2097152 bytes for committing reserved memory.
		# Possible reasons:
		#   The system is out of physical RAM or swap space
		#   In 32 bit mode, the process size limit was hit
		# Possible solutions:
		#   Reduce memory load on the system
		#   Increase physical memory or swap space
		#   Check if swap backing store is full
		#   Use 64 bit Java on a 64 bit OS
		#   Decrease Java heap size (-Xmx/-Xms)
		#   Decrease number of Java threads
		#   Decrease Java thread stack sizes (-Xss)
		#   Set larger code cache with -XX:ReservedCodeCacheSize=
		# This output file may be truncated or incomplete.
		#
		#  Out of Memory Error (os_linux.cpp:2761), pid=31426, tid=139549745604352
		#
		# JRE version: OpenJDK Runtime Environment (7.0_51-b02) (build 1.7.0_51-mockbuild_2014_01_15_01_39-b00)
		# Java VM: OpenJDK 64-Bit Server VM (24.45-b08 mixed mode linux-amd64 ) 
10. Spark automatically removes old RDDs from the cache when you make new ones. Unpersist forces it to remove them right away. In both cases though, note that Java doesn’t garbage-collect the objects released until later.

11. In my experiment, I have 20 machine, each machine own 2 executor, and I used the default parallelize, which is 8, so there  320  tasks in one stage in total.

	Then the workers will send 320*(400M/8)=16G data back to the driver, this seem very big. but I get from log that after serialize, the data size send back to driver is just 446 byte in each task. 
	
	broadcast is supposed to send data from the driver to the executors and not the other direction. 
	
	Size calculation is correct, but broadcast happens from the driver to the workers. 

	btw, your code is broadcasting 400MB 30 times, which are not being evicted from the cache fast enough, which, I think, is causing blockManagers to run out of memory.

12. BlockManager is like a distributed key-value store for large blobs (called blocks) of data. It has a master-worker architecture (loosely it is like the HDFS file system) where the BlockManager at the workers store the data blocks and BlockManagerMaster stores the metadata for what blocks are stored where. All the cached RDD's partitions and shuffle data are stored and managed by the BlockManager. It also transfers the blocks between the workers as needed (shuffles etc all happen through the block manager). Specifically for spark streaming, the data received from outside is stored in the BlockManager of the worker nodes, and the IDs of the blocks are reported to the BlockManagerMaster.

13. MapOutputTrackers is a simpler component that keeps track of the location of the output of the map stage, so that workers running the reduce stage knows which machines to pull the data from. That also has the master-worker component - master has the full knowledge of the mapoutput and the worker component on-demand pulls that knowledge from the master component when the reduce tasks are executed on the worker.

14. If you call DStream.persist (persist == cache = true), then all RDDs generated by the DStream will be persisted in the cache (in the BlockManager). As new RDDs are generated and persisted, old RDDs from the same DStream will fall out of memory. either by LRU or explicitly if spark.streaming.unpersist is set to true. 

15. In my experience, you don't need much horsepower on the master or worker nodes.  If you're bringing large data back to the driver (e.g. with .take or .collect) you can cause OOMs on the driver, so bump the heap if that's the case.  But the majority of your memory requirements will be in the executors, which are JVMs that the Worker spins up for each application (in the standalone mode cluster).

16. [Join : Giving incorrect result](http://apache-spark-user-list.1001560.n3.nabble.com/Join-Giving-incorrect-result-td6910.html#a7125)

	There was indeed a bug, specifically in the way join tasks spill to disk (which happened when you had more concurrent tasks competing for memory).
	
17. [create a self destructing iterator that releases records from hash maps](https://issues.apache.org/jira/browse/SPARK-2255)

	This is a small thing to do that can help out with GC pressure. For aggregations (and potentially joins), we don't really need to hold onto the key value pairs as soon as we have iterate over them. We can create a self destructing iterator for AppendOnlyMap / ExternalAppendOnlyMap that removes references to the key value pair as the iterator goes through records so those memory can be freed quickly.
	
	This is very similar to the memory management of aggregation in shuffles. 

	This mechanism relies on the accuracy of size estimation, however, which is not guaranteed. 


## Ideas
1. display and control the memory usage of code.
2. off-heap approach
3. weak references
4. let the error occurs in single node, then can help debug
5. test on small dataset
6. monitor the memory usage and estimate the data size and then decide to how to store the data
7. add counters
8. detection of memory-bloat
9. Sharing the available space between tasks

## Problem
1. data becomes large
2. configuration
